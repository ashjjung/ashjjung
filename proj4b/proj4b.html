<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Project 4A</title>
    <style>
        header {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px auto;
            max-width: 1000px; 
            color: #333;
            text-align: center; 
            font-size: 2.5em;  
            font-weight: bold;   
        }
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px auto;
            padding-left: 15px;
            max-width: 1000px; 
        }
        h1, h2 {
            margin: 20px auto;
            color: #333;
        }
        .image-container {
            display: flex;
            align-items: center;
            width: 80%; 
            margin: 0 auto; 
        }
        
        .image-container figure {
            margin: 0;
            flex: 1; 
            text-align: center;
            max-width: 80%; 
            margin: 10 10px;
        }
        figure {
            margin: 0;
            text-align: center;
        }
        img {
            max-width: 66%; 
            height: auto;
        }
        figcaption {
            margin: 3px 10px;
            font-size: 0.9em;
            color: #555;
        }
        .equation {
            font-family: 'Courier New', monospace;
            background: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            margin: 10px 0;
        }
        .fraction {
            display: inline-block;
            text-align: center;
        }
        .fraction .numerator {
            border-bottom: 1px solid #000;
            display: block;
            padding-bottom: 2px;
        }
        .fraction .denominator {
            display: block;
            padding-top: 2px;
        }
        p {
            margin: 20px 0;
        }
        @media (max-width: 800px) {
            .image-container {
                flex-direction: column;
                max-width: 100%;
            }
            .image-container figure {
                width: 100%;
                margin: 10px 0;
            }
        }
    </style>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
</head>

<body>
<header>Project 4 : (Auto)Stitching Photo Mosaics</header>
<h1>Part A - Image Warping and Mosaicing</h1>
<h2>Part 1: Shoot the Pictures</h2>
<p> I took photos with my phone from three distinct locations, rotating the camera at each spot.</p>
<p> Original images:</p>
<div class="image-container">
    <figure>
        <img src="data/a.jpg" alt="a">
    </figure>
    <figure>
        <img src="data/b.jpg" alt="b">
    </figure>
</div>

<div class="image-container">
    <figure>
        <img src="data/c.jpg" alt="c">
    </figure>
    <figure>
        <img src="data/d.jpg" alt="d">
    </figure>
</div>

<div class="image-container">
    <figure>
        <img src="data/g.jpg" alt="g">
    </figure>
    <figure>
        <img src="data/h.jpg" alt="h">
    </figure>
</div>
<h2>Part 2: Recover Homography Matrices</h2>
<h3>Homography Equation</h3>
    <p>
        A homography matrix \( H \) relates points in one image to corresponding points in another image:
    </p>
    <div class="equation">
        \[
        \begin{bmatrix}
        x' \\
        y' \\
        1
        \end{bmatrix}
        =
        H \cdot
        \begin{bmatrix}
        x \\
        y \\
        1
        \end{bmatrix}
        \]
    </div>
    
    <p>Expanding above,</p>
    <div class="equation">
        \[
        x' = \frac{H[0,0] \cdot x + H[0,1] \cdot y + H[0,2]}{H[2,0] \cdot x + H[2,1] \cdot y + H[2,2]}
        \]
        \[
        y' = \frac{H[1,0] \cdot x + H[1,1] \cdot y + H[1,2]}{H[2,0] \cdot x + H[2,1] \cdot y + H[2,2]}
        \]
    </div>

    <h3>Linear System Setup</h3>
    <p>
        For each pair of corresponding points \( (x, y) \) and \( (x', y') \), two equations can be set up:
    </p>
    <div class="equation">
        <p>\( H[0,0] \cdot x + H[0,1] \cdot y + H[0,2] - x' \cdot (H[2,0] \cdot x + H[2,1] \cdot y + H[2,2]) = 0 \)</p>
        <p>\( H[1,0] \cdot x + H[1,1] \cdot y + H[1,2] - y' \cdot (H[2,0] \cdot x + H[2,1] \cdot y + H[2,2]) = 0 \)</p>
    </div>

    <h3>Matrix Formulation</h3>
    <p>
        We build a matrix \( A \) from the equations above. For each point pair, we have:
    </p>
    <div class="equation">
        <p>\([x, y, 1, 0, 0, 0, -x' \cdot x, -x' \cdot y, -x']\)</p>
        <p>\([0, 0, 0, x, y, 1, -y' \cdot x, -y' \cdot y, -y']\)</p>
    </div>
    <p>
        This produces a system \( A \cdot h = 0 \), where \( h \) is a vector of the 8 unknowns in \( H \).
    </p>

    <h3>Solving Using SVD</h3>
    <p>
        To solve for \( H \), I used Singular Value Decomposition (SVD) to estimate Homography Matrix. 
    </p>

<h2>Part 3: Image Warping</h2>
<p>
    The <code>warpImage</code> function takes an image and a homography matrix <code>H</code> as inputs to warp the image according to the transformation described by <code>H</code>. The function defines the four corners of the image and applies the inverse of the homography matrix to transform these corners, calculating the minimum and maximum x and y coordinates to determine the size of the output canvas. If the transformed coordinates extend beyond the image boundaries, it calculates offsets to ensure the entire transformed image fits within the new canvas dimensions.
</p>
<p>
    The function uses these transformed coordinates to generate a polygon that encompasses the new boundaries. It then constructs the homogeneous coordinates of points within this polygon and maps them back to the source image using the homography matrix. To perform this transformation, the function uses inverse mapping and normalizes the coordinates.
</p>
<p>
    Next, it creates a grid of points corresponding to the pixel locations in the source image, and initializes an empty canvas for the warped image with the computed size. The function then applies <code>scipy.interpolate.griddata</code> to interpolate pixel values from the source image for each channel individually. It uses the nearest neighbor method to fill the pixels in the warped image based on the transformed coordinates.
</p>
<p>
    Finally, the function assigns the interpolated pixel values to the appropriate locations in the new canvas. 
</p>
<p> Here are the wraped results: </p>
<div class="image-container">
    <figure>
        <img src="output/b_warp.jpg" alt="b_warp">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/d_warp.jpg" alt="d_warp">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/h_warp.jpg" alt="h_warp">
    </figure>
</div>

<h2>Part 4: Rectification</h2>
<p>
    To rectify images, the <code>warpImage</code> function is used with inverse warping. Inverse warping involves applying the inverse of the homography matrix, which transforms the coordinates of the distorted image back to their original, undistorted positions. 
</p>
<p>
    By using inverse warping, the function interpolates pixel values from the source image to fit a new, rectified view. The result is an image that appears as if it were taken from a frontal perspective.
</p>
<div class="image-container">
    <figure>
        <img src="data/e.jpg" alt="rectify1">
    </figure>
    <figure>
        <img src="output/rectify1.jpg" alt="rectify2">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="data/f.jpg" alt="rectify1">
    </figure>
    <figure>
        <img src="output/rectify2.jpg" alt="rectify2">
    </figure>
</div>


<h2>Part 5: Blend the images into a mosaic</h2>
<p>
    This process combines two images using Laplacian stack from project 2. The blending process involves the following steps:
</p>
<ol>
    <li>
        <strong>Image Warping:</strong> The second image is warped using <code>warpImage</code> function from previous parts.
    </li>
    <li>
        <strong>Canvas Creation:</strong> A larger canvas is created to accommodate both the first and the warped second image. 
    </li>
    <li>
        <strong>Mask Generation:</strong> A mask is generated to differentiate between regions where only the first image, only the second image, and the overlapping areas exist. In the overlapping regions, a gradient is created using distance transforms, which allows for a smooth transition between the two images.
    </li>
    <li>
        <strong>Gaussian and Laplacian Pyramids:</strong> Gaussian and Laplacian pyramids are created for both images. The mask is also processed with Gaussian pyramids to match the levels of the image pyramids.
    </li>
    <li>
        <strong>Blending Process:</strong> The images are blended using the pyramids and the generated mask. The mask ensures a smooth transition in the overlapping region, where the blending weights vary gradually from one image to the other based on the gradient values.
    </li>
</ol>
<p>
    Here are final results:
</p>
<div class="image-container">
    <figure>
        <img src="output/ab_merge.jpg" alt="ab_merge">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/cd_merge.jpg" alt="cd_merge">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/gh_merge.jpg" alt="gh_merge">
    </figure>
</div>
<h2>Reflection</h2>
<p> Working with front warping and inverse warping was tricky and required careful consideration of coordinate transformations. It took a significant amount of time to debug, but it was interesting overall as I could experiment with distorting images from different perspectives. 
</p>
</br>

<h1>Part B - Feature Matching for Autostitching</h1>
<h2>Part 1: Harris Interest Point Detector</h2>
<p>The <strong>Harris Corner Detector</strong> identifies corners by finding points in an image where the intensity changes significantly in both the <em>x</em> and <em>y</em> directions.</p>

<h3>Process Overview:</h3>
<ol>
    <li><strong>Compute Gradients</strong>: Calculate the intensity changes (gradients) in the <em>x</em> and <em>y</em> directions.</li>
    <li><strong>Corner Response</strong>: Use these gradients to compute a corner response score, which highlights likely corner points.</li>
    <li><strong>Thresholding</strong>: Keep only points with high corner scores.</li>
</ol>

<p>I used the provided <code>harris.py</code> code to apply this process. Below is a figure showing the Harris corners overlaid on the image.</p>
<div class="image-container">
    <figure>
        <img src="output/harris_a.jpg" alt="harris_a">
    </figure>
    <figure>
        <img src="output/harris_b.jpg" alt="harris_b">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/harris_c.jpg" alt="harris_c">
    </figure>
    <figure>
        <img src="output/harris_d.jpg" alt="harris_d">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/harris_g.jpg" alt="harris_e">
    </figure>
    <figure>
        <img src="output/harris_h.jpg" alt="harris_f">
    </figure>
</div>
<h2>Part 2: Adaptive Non-Maximal Suppression </h2>
<h3>Approach:</h3>
<ol>
    <li><strong>Compute Corner Responses</strong>: For each point, retrieve the Harris corner response (strength) from <code>h</code> based on the input coordinates <code>coords</code>.</li>
    
    <li><strong>Sort by Response Strength</strong>: Sort the coordinates in descending order of their corner response values.</li>
    
    <li><strong>Calculate Radius for Suppression</strong>: For each point, calculate a radius representing the minimum distance to any higher-ranked point. This radius is constrained by <code>c_robust=0.9</code>, ensuring that only points with significantly lower responses are suppressed.</li>
    
    <li><strong>Select Top Features</strong>: Choose the points with the largest radii up to the specified maximum number of features <code>max_features=250</code>. These points represent the most robust features with good spatial distribution.</li>
</ol>

<p>The <code>anms</code> function effectively filters out non-robust features while keeping the well-distributed corners. <br>
    The results below show the key points selected through ANMS:</p>
<div class="image-container">
    <figure>
        <img src="output/anms_a.jpg" alt="anms_a">
    </figure>
    <figure>
        <img src="output/anms_b.jpg" alt="anms_b">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/anms_c.jpg" alt="anms_c">
    </figure>
    <figure>
        <img src="output/anms_d.jpg" alt="anms_d">
    </figure>
</div>
<div class="image-container">
    <figure>
        <img src="output/anms_g.jpg" alt="anms_g">
    </figure>
    <figure>
        <img src="output/anms_h.jpg" alt="anms_h">
    </figure>
</div>
<h2>Part 3: Feature Descriptor extraction</h2>
<h3>Approach:</h3>
<ol>
    <li><strong>Patch Extraction</strong>: For each interest point, a <code>40x40</code> patch centered on the point is extracted from the image.</li>
    
    <li><strong>Downsampling to 8x8</strong>: The patch is downsampled to an <code>8x8</code> patch.</li>
    
    <li><strong>Normalization</strong>: The extracted patch is bias/gain-normalized by subtracting the mean and dividing by the standard deviation.</li>
    <p> The resulting feature descriptors are used to match the set of images.</p>
</ol>

<p> Below are the extracted and normalized patches of one of the images:</p>
<div class="image-container">
    <figure>
    <img src="output/feature_desc.jpg" alt="feature_desc">
    </figure>
</div>

<h2>Part 4: Feature Matching</h2>
<p>The feature matching process compares descriptors from two images to identify corresponding features.</p>

<h3>Approach:</h3>
<ol>
    <li><strong>Flatten Descriptors</strong>: The descriptors from both images are flattened for easier distance calculation, transforming each descriptor into a 1D vector.</li>
    
    <li><strong>Distance Calculation</strong>: For each descriptor in <code>descriptor1</code> (from Image 1), the Euclidean distance to every descriptor in <code>descriptor2</code> (from Image 2) is computed.</li>
    
    <li><strong>Find Closest Matches</strong>: The two descriptors with the smallest distances are identified. The nearest descriptor (closest match) and the second nearest are used for comparison.</li>
    
    <li><strong>Lowe's Ratio Test</strong>: If the distance to the nearest match is smaller (within the <code>ratio_threshold</code>) than the distance to the second nearest match, the match is retained.</li>
    
    <li><strong>Store Matches</strong>: The matched key points are stored.</li>
</ol>

<p>This matching process yields pairs of points from both images that are likely to correspond. <br>
    Here are the results of the matched features across the set of images:</p>

    <div style="text-align: center;">
        <img src="output/matched_ab.jpg" alt="matched_ab">
    </div>
    <div style="text-align: center;">
        <img src="output/matched_cd.jpg" alt="matched_cd">
    </div>
    <div style="text-align: center;">
        <img src="output/matched_gh.jpg" alt="matched_gh">
    </div>

<h2>Part 5: RANSAC</h2> 
    <h3>Approach:</h3>
    <ol>
        <li><strong>Random Sampling of Feature Pairs</strong>: In each iteration, four feature pairs are randomly selected from the points in both images.</li>
        
        <li><strong>Compute Homography Matrix</strong>: A homography matrix <code>H</code> is computed using the selected pairs.</li>
        
        <li><strong>Calculate Inliers</strong>: Using <code>H</code>, points from the first image are transformed, and their distances to corresponding points in the second image are calculated. If the distance is below a threshold, the point pair is considered an inlier.</li>
        
        <li><strong>Identify Best Inliers</strong>: The set of inliers is tracked across all iterations, and the <code>H</code> with the largest inlier set is retained as the best fit.</li>
        
        <li><strong>Recompute Final Homography</strong>: After finding the best set of inliers, a final <code>H</code> is computed using all inliers to increase accuracy.</li>
    </ol>
    
    <p>By iteratively refining the homography based on inliers, RANSAC provides a robust estimate of the transformation.<br>
        Below are the results of the RANSAC process across the set of images:</p>
        <div style="text-align: center;">
            <img src="output/ransac_ab.jpg" alt="ransac_ab">
        </div>
        <div style="text-align: center;">
            <img src="output/ransac_cd.jpg" alt="ransac_cd">
        </div>
        <div style="text-align: center;">
            <img src="output/ransac_gh.jpg" alt="ransac_gh">
        </div>
    <h2>Mosaics</h2> 
    <p>Below are the results of three mosaics: automatically stitched and manually stitched, respectively.</p>
    <div class="image-container">
        <figure>
            <img src="output/autostitch_ab.jpg" alt="autostitch_ab">
        </figure>
        <figure>
            <img src="output/ab_merge.jpg" alt="ab_merge">
        </figure>
    </div>
    <div class="image-container">
        <figure>
            <img src="output/autostitch_cd.jpg" alt="autostitch_cd">
        </figure>
        <figure>
            <img src="output/cd_merge.jpg" alt="cd_merge">
        </figure>
    </div>
    <div class="image-container">
        <figure>
            <img src="output/autostitch_gh.jpg" alt="autostitch_gh">
        </figure>
        <figure>
            <img src="output/gh_merge.jpg" alt="gh_merge">
        </figure>
    </div>
    <p> There is not a significant visual difference between the manually stitched and automatically stitched versions. Both methods align the images well. The automatic stitching algorithm effectively aligns key features without noticeable misalignments.</p>

</body>
</html>